{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from redhat_documentation import RedHatDocumentationLoader\n",
    "from langchain_community.document_transformers import Html2TextTransformer\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: Introduction to Red Hat OpenShift AI\n"
     ]
    }
   ],
   "source": [
    "urls = [\"https://access.redhat.com/documentation/en-us/red_hat_openshift_ai_self-managed/2-latest/html-single/introduction_to_red_hat_openshift_ai/index\"]\n",
    "loader = RedHatDocumentationLoader(urls)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='# Chapter 1. Overview of OpenShift AI\\n\\nUsing Red Hat OpenShift AI, users can integrate data, artificial intelligence\\nand machine learning software to execute end-to-end machine learning\\nworkflows. OpenShift AI is supported in two configurations: 1\\\\. Installed as\\nan Add-on to a Red Hat managed environment such as Red Hat OpenShift Dedicated\\nand Red Hat OpenShift Service on Amazon Web Services (ROSA). 2\\\\. Installed as\\na self-managed Operator on a self-managed environment, such as Red Hat\\nOpenShift Container Platform. For data scientists, OpenShift AI includes\\nJupyter and a collection of default notebook images optimized with the tools\\nand libraries required for model development, and the TensorFlow and PyTorch\\nframeworks. Deploy and host your models, integrate models into external\\napplications, and export models to host them in any hybrid cloud environment.\\nYou can also accelerate your data science experiments through the use of\\ngraphics processing units (GPUs) and Habana Gaudi devices. For administrators,\\nOpenShift AI enables data science workloads in an existing Red Hat OpenShift\\nor ROSA environment. Manage users with your existing OpenShift identity\\nprovider, and manage the resources available to notebook servers to ensure\\ndata scientists have what they require to create, train, and host models. Use\\naccelerators to reduce costs and allow your data scientists to enhance the\\nperformance of their end-to-end data science workflows using graphics\\nprocessing units (GPUs) and Habana Gaudi devices. OpenShift AI offers two\\ndistributions: \\\\- A managed cloud service add-on for Red Hat OpenShift\\nDedicated (with a Customer Cloud Subscription for AWS or GCP) or for Red Hat\\nOpenShift Service on Amazon Web Services (ROSA).For information about\\nOpenShift AI on a Red Hat managed environment, see Product Documentation for\\nRed Hat OpenShift AI. \\\\- Self-managed software that you can install on-premise\\nor on the public cloud in a self-managed environment, such as OpenShift\\nContainer Platform.For information about OpenShift AI as self-managed software\\non your OpenShift cluster in a connected or a disconnected environment, see\\nProduct Documentation for Red Hat OpenShift AI Self-Managed. For information\\nabout OpenShift AI supported software platforms, components, and dependencies,\\nsee Supported configurations.\\n\\n# Chapter 2. Product features\\n\\nRed Hat OpenShift AI provides several features for data scientists and IT\\noperations administrators.\\n\\n## 2.1. Features for data scientists\\n\\n#### Containers\\n\\nWhile tools such as JupyterLab already offer intuitive ways for data\\nscientists to develop models on their machines, there are always inherent\\ncomplexities involved with collaboration and sharing work. Moreover, using\\nspecialized hardware such as powerful GPUs can be very expensive when you have\\nto buy and maintain your own. The Jupyter environment that is included with\\nOpenShift AI lets you take your development environment anywhere you need it\\nto be. Because all of the workloads are run as containers, collaboration is as\\neasy as sharing an image with your team members, or even simply adding it to\\nthe list of default containers that they can use. As a result, GPUs and large\\namounts of memory are significantly more accessible, since you are no longer\\nlimited by what your laptop can support.\\n\\n#### Integration with third-party machine learning tools\\n\\nWe have all run into situations where our favorite tools or services do not\\nplay well with one another. OpenShift AI is designed with flexibility in mind.\\nYou can use a wide range of open source and third-party tools with OpenShift\\nAI. These tools support the complete machine learning lifecycle, from data\\nengineering and feature extraction to model deployment and management.\\n\\n#### Collaboration on notebooks with Git\\n\\nUse Jupyter’s Git interface to work collaboratively with others, and keep good\\ntrack of the changes to your code.\\n\\n#### Securely built notebook images\\n\\nChoose from a default set of notebook images that are pre-configured with the\\ntools and libraries that you need for model development. Software stacks,\\nespecially those involved in machine learning, tend to be complex systems.\\nThere are many modules and libraries in the Python ecosystem that can be used,\\nso determining which versions of what libraries to use can be very\\nchallenging. OpenShift AI includes many packaged notebook images that have\\nbeen built with insight from data scientists and recommendation engines. You\\ncan start new projects quickly on the right foot without worrying about\\ndownloading unproven and possibly insecure images from random upstream\\nrepositories.\\n\\n#### Custom notebooks\\n\\nIn addition to notebook images provided and supported by Red Hat and\\nindependent software vendors (ISVs), you can configure custom notebook images\\nthat cater to your project’s specific requirements.\\n\\n#### Data science pipelines\\n\\nOpenShift AI supports data science pipelines for a mature and efficient way of\\nrunning your data science workloads. You can standardize and automate machine\\nlearning workflows that enable you to develop and deploy your data science\\nmodels.\\n\\n#### Model serving\\n\\nAs a data scientist, you can deploy your trained machine-learning models to\\nserve intelligent applications in production. Deploying or serving a model\\nmakes the model’s functions available as a service endpoint that can be used\\nfor testing or integration into applications. You have much control over how\\nthis serving is performed.\\n\\n#### Optimize your data science models with accelerators\\n\\nIf you work with large data sets, you can optimize the performance of your\\ndata science models in OpenShift AI with NVIDIA graphics processing units\\n(GPUs) or Habana Gaudi devices. Accelerators enable you to scale your work,\\nreduce latency, and increase productivity.\\n\\n## 2.2. Features for IT Operations administrators\\n\\n#### Manage users with an identity provider\\n\\nOpenShift AI supports the same authentication systems as your OpenShift\\ncluster. By default, OpenShift AI is accessible to all users listed in your\\nidentity provider and those users do not need a separate set of credentials to\\naccess OpenShift AI. Optionally, you can limit the set of users who have\\naccess by creating an OpenShift group that specifies a subset of users. You\\ncan also create an OpenShift group that identifies the list of users who have\\nadministrator access to OpenShift AI.\\n\\n#### Manage resources with OpenShift\\n\\nUse your existing OpenShift knowledge to configure and manage resources for\\nyour OpenShift AI users.\\n\\n#### Control Red Hat usage data collection\\n\\nChoose whether to allow Red Hat to collect data about OpenShift AI usage in\\nyour cluster. Usage data collection is enabled by default when you install\\nOpenShift AI on your OpenShift cluster.\\n\\n#### Apply autoscaling to your cluster to reduce usage costs\\n\\nUse the cluster autoscaler to adjust the size of your cluster to meet its\\ncurrent needs and optimize costs.\\n\\n#### Manage resource usage by stopping idle notebooks\\n\\nReduce resource usage in your OpenShift AI deployment by automatically\\nstopping notebook servers that have been idle for a period of time.\\n\\n#### Implement model-serving runtimes\\n\\nOpenShift AI provides support for model-serving runtimes. A model-serving\\nruntime provides integration with a specified model server and the model\\nframeworks that it supports. By default, OpenShift AI includes the OpenVINO\\nModel Server runtime. However, if this runtime doesn’t meet your needs (for\\nexample, if it doesn’t support a particular model framework), you can add your\\nown custom runtimes.\\n\\n#### Install in a disconnected environment\\n\\nOpenShift AI Self-Managed supports installation in a disconnected environment.\\nDisconnected clusters are on a restricted network, typically behind a firewall\\nand unable to reach the Internet. In this case, clusters cannot access the\\nremote registries where Red Hat provided OperatorHub sources reside. In this\\ncase, you deploy the OpenShift AI Operator to a disconnected environment by\\nusing a private registry in which you have mirrored (copied) the relevant\\nimages.\\n\\n#### Manage accelerators\\n\\nEnable NVIDIA graphics processing units (GPUs) or Habana Gaudi devices in\\nOpenShift AI and allow your data scientists to use compute-heavy workloads.\\n\\n# Chapter 3. Try It\\n\\nData scientists and developers can try OpenShift AI and access tutorials and\\nactivities in the Red Hat Developer sandbox environment. IT operations\\nadministrators can try OpenShift AI in your own cluster with a 60-day product\\ntrial.\\n\\n# Chapter 4. Get It\\n\\n#### Managed cloud service\\n\\nYou have the following options for subscribing to OpenShift AI as a managed\\nservice: - For OpenShift Dedicated, subscribe through Red Hat. - For Red Hat\\nOpenShift Service on Amazon Web Services (ROSA), subscribe through Red Hat or\\nsubscribe through the AWS Marketplace.\\n\\n#### Self-managed software\\n\\nTo get Red Hat OpenShift AI as self-managed software, sign up for it with your\\nRed Hat account team.\\n\\n', metadata={'source': 'https://access.redhat.com/documentation/en-us/red_hat_openshift_ai_self-managed/2-latest/html-single/introduction_to_red_hat_openshift_ai/index', 'title': 'Introduction to Red Hat OpenShift AI'})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html2text = Html2TextTransformer()\n",
    "md_docs = html2text.transform_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='# Chapter 1. Overview of OpenShift AI  \\nUsing Red Hat OpenShift AI, users can integrate data, artificial intelligence\\nand machine learning software to execute end-to-end machine learning\\nworkflows. OpenShift AI is supported in two configurations: 1\\\\. Installed as\\nan Add-on to a Red Hat managed environment such as Red Hat OpenShift Dedicated\\nand Red Hat OpenShift Service on Amazon Web Services (ROSA). 2\\\\. Installed as\\na self-managed Operator on a self-managed environment, such as Red Hat\\nOpenShift Container Platform. For data scientists, OpenShift AI includes\\nJupyter and a collection of default notebook images optimized with the tools\\nand libraries required for model development, and the TensorFlow and PyTorch\\nframeworks. Deploy and host your models, integrate models into external\\napplications, and export models to host them in any hybrid cloud environment.\\nYou can also accelerate your data science experiments through the use of\\ngraphics processing units (GPUs) and Habana Gaudi devices. For administrators,\\nOpenShift AI enables data science workloads in an existing Red Hat OpenShift\\nor ROSA environment. Manage users with your existing OpenShift identity\\nprovider, and manage the resources available to notebook servers to ensure\\ndata scientists have what they require to create, train, and host models. Use\\naccelerators to reduce costs and allow your data scientists to enhance the\\nperformance of their end-to-end data science workflows using graphics\\nprocessing units (GPUs) and Habana Gaudi devices. OpenShift AI offers two\\ndistributions: \\\\- A managed cloud service add-on for Red Hat OpenShift\\nDedicated (with a Customer Cloud Subscription for AWS or GCP) or for Red Hat\\nOpenShift Service on Amazon Web Services (ROSA).For information about\\nOpenShift AI on a Red Hat managed environment, see Product Documentation for\\nRed Hat OpenShift AI. \\\\- Self-managed software that you can install on-premise\\nor on the public cloud in a self-managed environment, such as OpenShift\\nContainer Platform.For information about OpenShift AI as self-managed software\\non your OpenShift cluster in a connected or a disconnected environment, see\\nProduct Documentation for Red Hat OpenShift AI Self-Managed. For information\\nabout OpenShift AI supported software platforms, components, and dependencies,\\nsee Supported configurations.', metadata={'Header 1': 'Chapter 1. Overview of OpenShift AI'}),\n",
       " Document(page_content='# Chapter 2. Product features  \\nRed Hat OpenShift AI provides several features for data scientists and IT\\noperations administrators.', metadata={'Header 1': 'Chapter 2. Product features'}),\n",
       " Document(page_content='## 2.1. Features for data scientists  \\n#### Containers  \\nWhile tools such as JupyterLab already offer intuitive ways for data\\nscientists to develop models on their machines, there are always inherent\\ncomplexities involved with collaboration and sharing work. Moreover, using\\nspecialized hardware such as powerful GPUs can be very expensive when you have\\nto buy and maintain your own. The Jupyter environment that is included with\\nOpenShift AI lets you take your development environment anywhere you need it\\nto be. Because all of the workloads are run as containers, collaboration is as\\neasy as sharing an image with your team members, or even simply adding it to\\nthe list of default containers that they can use. As a result, GPUs and large\\namounts of memory are significantly more accessible, since you are no longer\\nlimited by what your laptop can support.  \\n#### Integration with third-party machine learning tools  \\nWe have all run into situations where our favorite tools or services do not\\nplay well with one another. OpenShift AI is designed with flexibility in mind.\\nYou can use a wide range of open source and third-party tools with OpenShift\\nAI. These tools support the complete machine learning lifecycle, from data\\nengineering and feature extraction to model deployment and management.  \\n#### Collaboration on notebooks with Git  \\nUse Jupyter’s Git interface to work collaboratively with others, and keep good\\ntrack of the changes to your code.  \\n#### Securely built notebook images  \\nChoose from a default set of notebook images that are pre-configured with the\\ntools and libraries that you need for model development. Software stacks,\\nespecially those involved in machine learning, tend to be complex systems.\\nThere are many modules and libraries in the Python ecosystem that can be used,\\nso determining which versions of what libraries to use can be very\\nchallenging. OpenShift AI includes many packaged notebook images that have\\nbeen built with insight from data scientists and recommendation engines. You\\ncan start new projects quickly on the right foot without worrying about\\ndownloading unproven and possibly insecure images from random upstream\\nrepositories.  \\n#### Custom notebooks  \\nIn addition to notebook images provided and supported by Red Hat and\\nindependent software vendors (ISVs), you can configure custom notebook images\\nthat cater to your project’s specific requirements.  \\n#### Data science pipelines  \\nOpenShift AI supports data science pipelines for a mature and efficient way of\\nrunning your data science workloads. You can standardize and automate machine\\nlearning workflows that enable you to develop and deploy your data science\\nmodels.  \\n#### Model serving  \\nAs a data scientist, you can deploy your trained machine-learning models to\\nserve intelligent applications in production. Deploying or serving a model\\nmakes the model’s functions available as a service endpoint that can be used\\nfor testing or integration into applications. You have much control over how\\nthis serving is performed.  \\n#### Optimize your data science models with accelerators  \\nIf you work with large data sets, you can optimize the performance of your\\ndata science models in OpenShift AI with NVIDIA graphics processing units\\n(GPUs) or Habana Gaudi devices. Accelerators enable you to scale your work,\\nreduce latency, and increase productivity.', metadata={'Header 1': 'Chapter 2. Product features', 'Header 2': '2.1. Features for data scientists'}),\n",
       " Document(page_content='## 2.2. Features for IT Operations administrators  \\n#### Manage users with an identity provider  \\nOpenShift AI supports the same authentication systems as your OpenShift\\ncluster. By default, OpenShift AI is accessible to all users listed in your\\nidentity provider and those users do not need a separate set of credentials to\\naccess OpenShift AI. Optionally, you can limit the set of users who have\\naccess by creating an OpenShift group that specifies a subset of users. You\\ncan also create an OpenShift group that identifies the list of users who have\\nadministrator access to OpenShift AI.  \\n#### Manage resources with OpenShift  \\nUse your existing OpenShift knowledge to configure and manage resources for\\nyour OpenShift AI users.  \\n#### Control Red Hat usage data collection  \\nChoose whether to allow Red Hat to collect data about OpenShift AI usage in\\nyour cluster. Usage data collection is enabled by default when you install\\nOpenShift AI on your OpenShift cluster.  \\n#### Apply autoscaling to your cluster to reduce usage costs  \\nUse the cluster autoscaler to adjust the size of your cluster to meet its\\ncurrent needs and optimize costs.  \\n#### Manage resource usage by stopping idle notebooks  \\nReduce resource usage in your OpenShift AI deployment by automatically\\nstopping notebook servers that have been idle for a period of time.  \\n#### Implement model-serving runtimes  \\nOpenShift AI provides support for model-serving runtimes. A model-serving\\nruntime provides integration with a specified model server and the model\\nframeworks that it supports. By default, OpenShift AI includes the OpenVINO\\nModel Server runtime. However, if this runtime doesn’t meet your needs (for\\nexample, if it doesn’t support a particular model framework), you can add your\\nown custom runtimes.  \\n#### Install in a disconnected environment  \\nOpenShift AI Self-Managed supports installation in a disconnected environment.\\nDisconnected clusters are on a restricted network, typically behind a firewall\\nand unable to reach the Internet. In this case, clusters cannot access the\\nremote registries where Red Hat provided OperatorHub sources reside. In this\\ncase, you deploy the OpenShift AI Operator to a disconnected environment by\\nusing a private registry in which you have mirrored (copied) the relevant\\nimages.  \\n#### Manage accelerators  \\nEnable NVIDIA graphics processing units (GPUs) or Habana Gaudi devices in\\nOpenShift AI and allow your data scientists to use compute-heavy workloads.', metadata={'Header 1': 'Chapter 2. Product features', 'Header 2': '2.2. Features for IT Operations administrators'}),\n",
       " Document(page_content='# Chapter 3. Try It  \\nData scientists and developers can try OpenShift AI and access tutorials and\\nactivities in the Red Hat Developer sandbox environment. IT operations\\nadministrators can try OpenShift AI in your own cluster with a 60-day product\\ntrial.', metadata={'Header 1': 'Chapter 3. Try It'}),\n",
       " Document(page_content='# Chapter 4. Get It  \\n#### Managed cloud service  \\nYou have the following options for subscribing to OpenShift AI as a managed\\nservice: - For OpenShift Dedicated, subscribe through Red Hat. - For Red Hat\\nOpenShift Service on Amazon Web Services (ROSA), subscribe through Red Hat or\\nsubscribe through the AWS Marketplace.  \\n#### Self-managed software  \\nTo get Red Hat OpenShift AI as self-managed software, sign up for it with your\\nRed Hat account team.', metadata={'Header 1': 'Chapter 4. Get It'})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split_on,\n",
    "    strip_headers=False\n",
    "    )\n",
    "md_header_splits = markdown_splitter.split_text(docs_transformed[0].page_content)\n",
    "md_header_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header: {'Header 1': 'Chapter 1. Overview of OpenShift AI'}\n",
      "Content: # Chapter 1. Overview of OpenShift AI  \n",
      "Using Red Hat OpenShift AI, users can integrate data, artificial intelligence\n",
      "and machine learning software to execute end-to-end machine learning\n",
      "workflows. OpenShift AI is supported in two configurations: 1\\. Installed as\n",
      "an Add-on to a Red Hat managed environment such as Red Hat OpenShift Dedicated\n",
      "and Red Hat OpenShift Service on Amazon Web Services (ROSA). 2\\. Installed as\n",
      "a self-managed Operator on a self-managed environment, such as Red Hat\n",
      "OpenShif\n",
      "\n",
      "\n",
      "\n",
      "Header: {'Header 1': 'Chapter 2. Product features'}\n",
      "Content: # Chapter 2. Product features  \n",
      "Red Hat OpenShift AI provides several features for data scientists and IT\n",
      "operations administrators.\n",
      "\n",
      "\n",
      "\n",
      "Header: {'Header 1': 'Chapter 2. Product features', 'Header 2': '2.1. Features for data scientists'}\n",
      "Content: ## 2.1. Features for data scientists  \n",
      "#### Containers  \n",
      "While tools such as JupyterLab already offer intuitive ways for data\n",
      "scientists to develop models on their machines, there are always inherent\n",
      "complexities involved with collaboration and sharing work. Moreover, using\n",
      "specialized hardware such as powerful GPUs can be very expensive when you have\n",
      "to buy and maintain your own. The Jupyter environment that is included with\n",
      "OpenShift AI lets you take your development environment anywhere you n\n",
      "\n",
      "\n",
      "\n",
      "Header: {'Header 1': 'Chapter 2. Product features', 'Header 2': '2.2. Features for IT Operations administrators'}\n",
      "Content: ## 2.2. Features for IT Operations administrators  \n",
      "#### Manage users with an identity provider  \n",
      "OpenShift AI supports the same authentication systems as your OpenShift\n",
      "cluster. By default, OpenShift AI is accessible to all users listed in your\n",
      "identity provider and those users do not need a separate set of credentials to\n",
      "access OpenShift AI. Optionally, you can limit the set of users who have\n",
      "access by creating an OpenShift group that specifies a subset of users. You\n",
      "can also create an OpenShi\n",
      "\n",
      "\n",
      "\n",
      "Header: {'Header 1': 'Chapter 3. Try It'}\n",
      "Content: # Chapter 3. Try It  \n",
      "Data scientists and developers can try OpenShift AI and access tutorials and\n",
      "activities in the Red Hat Developer sandbox environment. IT operations\n",
      "administrators can try OpenShift AI in your own cluster with a 60-day product\n",
      "trial.\n",
      "\n",
      "\n",
      "\n",
      "Header: {'Header 1': 'Chapter 4. Get It'}\n",
      "Content: # Chapter 4. Get It  \n",
      "#### Managed cloud service  \n",
      "You have the following options for subscribing to OpenShift AI as a managed\n",
      "service: - For OpenShift Dedicated, subscribe through Red Hat. - For Red Hat\n",
      "OpenShift Service on Amazon Web Services (ROSA), subscribe through Red Hat or\n",
      "subscribe through the AWS Marketplace.  \n",
      "#### Self-managed software  \n",
      "To get Red Hat OpenShift AI as self-managed software, sign up for it with your\n",
      "Red Hat account team.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for split in md_header_splits:\n",
    "    print(f\"Header: {split.metadata}\")\n",
    "    print(f\"Content: {split.page_content[:500]}\")\n",
    "    print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
